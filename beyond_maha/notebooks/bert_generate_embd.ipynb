{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os \n",
                "\n",
                "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "import torch\n",
                "from datasets import load_dataset\n",
                "from torch import nn\n",
                "from tqdm import tqdm, trange\n",
                "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "\n",
                "from torch.utils.data import DataLoader\n",
                "import pickle\n",
                "from src import AI_IRW, OODDetector\n",
                "\n",
                "from scipy.spatial.distance import mahalanobis, cdist\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import seaborn as sns\n",
                "sns.set_style('white')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(\"fabriceyhc/bert-base-uncased-imdb\")\n",
                "\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-imdb\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Found cached dataset imdb (/home/onyxia/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7c0861ce8e1b42eba9eb2a71ee24974f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/3 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "dataset = load_dataset(\"imdb\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LatentBert(nn.Module):\n",
                "\n",
                "    def __init__(self, base_model):\n",
                "        super().__init__()\n",
                "        self.base_model = base_model\n",
                "        self.N = len(self.base_model.bert.encoder.layer)\n",
                "\n",
                "    def to(self, device):\n",
                "        self.base_model.to(device)\n",
                "        return self\n",
                "\n",
                "    def forward(self, x, aggregated=True):\n",
                "        \n",
                "        x = self.base_model.bert.embeddings(x)\n",
                "        latent_reps = torch.zeros((x.shape[0], x.shape[2],self.N)\n",
                "        )\n",
                "        for i, layer in enumerate(self.base_model.bert.encoder.layer) :\n",
                "            x = layer(x)[0]\n",
                "            latent_reps[:,:,i] = x[:,0,:] #Pooling is done by taking the hidden state of first token (cf Bert implementation)\n",
                "        \n",
                "        x_ = self.base_model.bert.pooler(x)\n",
                "        x = self.base_model.classifier(x_)\n",
                "                \n",
                "        return {\"embeddings\" : torch.mean(latent_reps, axis=-1) if aggregated else latent_reps, \"logits\": x, \"attention\": x_}\n",
                "        "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "lb = LatentBert(base_model=model).to('cuda')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_in_dl = DataLoader(dataset['train'], batch_size=8)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "environ 14min"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 3125/3125 [12:14<00:00,  4.25it/s]\n"
                    ]
                }
            ],
            "source": [
                "embds = []\n",
                "logits = []\n",
                "attns = []\n",
                "for i,batch in enumerate(tqdm(train_in_dl)):\n",
                "    data = batch['text']\n",
                "    tk_batch = tokenizer(data,return_tensors=\"pt\", truncation=True, padding=True)['input_ids'].to('cuda')\n",
                "    outputs = lb.forward(tk_batch, aggregated=False)\n",
                "    embds.append(outputs['embeddings'].cpu().detach())\n",
                "    logits.append(outputs['logits'].cpu().detach())\n",
                "    attns.append(outputs['attention'].cpu().detach())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('../nlp_data/embeddings_imbd_train.pkl', 'wb') as f:\n",
                "    pickle.dump(embds, f)\n",
                "\n",
                "with open('../nlp_data/logits_imdb_train.pkl', 'wb') as f:\n",
                "    pickle.dump(logits, f)\n",
                "    \n",
                "with open('../nlp_data/attns_imdb_train.pkl', 'wb') as f:\n",
                "    pickle.dump(attns, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "oods = dict(multi30k = [\"nlphuji/flickr30k\"],\n",
                "sst2 = [\"sst2\"],\n",
                "wmt16 = ['wmt16', 'de-en'],\n",
                "newsgroup = [\"SetFit/20_newsgroups\"])\n",
                "\n",
                "collate_fns = dict(\n",
                "    multi30k= lambda x : x[1]['caption'][0], \n",
                "    sst2= lambda x: [el['sentence'] for el in x], \n",
                "    wmt16 = lambda x : [el['translation']['en'] for el in x], \n",
                "    newsgroup=lambda x : [el['text'].replace('\\n', ' ').replace(\"\\\\\", '') for el in x ]\n",
                ")\n",
                "\n",
                "batch_sizes = dict(\n",
                "    multi30k= 16, \n",
                "    sst2= 16, \n",
                "    wmt16 = 16, \n",
                "    newsgroup=4\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Found cached dataset flickr30k (/home/onyxia/.cache/huggingface/datasets/nlphuji___flickr30k/TEST/1.1.0/6adb9ab2367c57c3e81e76ecaecb8047ea00c33dccf9da10455037f32ec43382)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f331441d383b401c9e6aaf83ad63d8cd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1939/1939 [07:08<00:00,  4.52it/s]\n",
                        "Found cached dataset sst2 (/home/onyxia/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3c898d87dd7740838d7fe43a65e132a5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/3 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 114/114 [00:05<00:00, 22.60it/s]\n",
                        "Found cached dataset wmt16 (/home/onyxia/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e4cc0cde00234bb99a082e820bdb7f2e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/3 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 188/188 [00:08<00:00, 20.92it/s]\n",
                        "Found cached dataset json (/home/onyxia/.cache/huggingface/datasets/SetFit___json/SetFit--20_newsgroups-f9362e018b6adf67/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e374b15f4dac4beda4b9a98a58b59586",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1883/1883 [02:46<00:00, 11.34it/s]\n"
                    ]
                }
            ],
            "source": [
                "for ds_n, ds in oods.items():\n",
                "    out_dataset  =  load_dataset(*ds)\n",
                "    ood_dl = DataLoader(dataset=out_dataset['test'],\n",
                "                        batch_size=batch_sizes[ds_n], \n",
                "                        collate_fn= collate_fns[ds_n], #for flickr\n",
                "                        )\n",
                "    embds = []\n",
                "    logits = []\n",
                "    attns = []\n",
                "    for batch in tqdm(ood_dl):\n",
                "        tk_batch = tokenizer(batch,return_tensors=\"pt\", truncation=True, padding=True)['input_ids'].to('cuda')\n",
                "        outputs = lb.forward(tk_batch, aggregated=False)\n",
                "        embds.append(outputs['embeddings'].cpu().detach())\n",
                "        logits.append(outputs['logits'].cpu().detach())\n",
                "        attns.append(outputs['attention'].cpu().detach())\n",
                "\n",
                "    with open(f'../nlp_data/embeddings_ood_test_{ds_n}.pkl', 'wb') as f:\n",
                "        pickle.dump(embds, f)\n",
                "\n",
                "    with open(f'../nlp_data/logits_ood_test_{ds_n}.pkl', 'wb') as f:\n",
                "        pickle.dump(logits, f)\n",
                "\n",
                "    with open(f'../nlp_data/attns_ood_test_{ds_n}.pkl', 'wb') as f:\n",
                "        pickle.dump(attns, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_in_dl = DataLoader(dataset['test'], batch_size=8)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "environ 14 min"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 3125/3125 [12:07<00:00,  4.29it/s]\n"
                    ]
                }
            ],
            "source": [
                "embds = []\n",
                "logits = []\n",
                "attns = []\n",
                "for i, batch in enumerate(tqdm(test_in_dl)):\n",
                "    data = batch['text']\n",
                "    tk_batch = tokenizer(data,return_tensors=\"pt\", truncation=True, padding=True)['input_ids'].to('cuda')\n",
                "    outputs = lb.forward(tk_batch, aggregated=False)\n",
                "    embds.append(outputs['embeddings'].cpu().detach())\n",
                "    logits.append(outputs['logits'].cpu().detach())\n",
                "    attns.append(outputs['attention'].cpu().detach())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('../nlp_data/embeddings_raw_test.pkl', 'wb') as f:\n",
                "    pickle.dump(embds, f)\n",
                "\n",
                "with open('../nlp_data/logits_test.pkl', 'wb') as f:\n",
                "    pickle.dump(logits, f)\n",
                "\n",
                "with open('../nlp_data/attns_test.pkl', 'wb') as f:\n",
                "    pickle.dump(attns, f)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "3fa046f995eb80ac40c0869a1f9df46519f4ada8b8c395ef25dd1aa1a1a2fc63"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
